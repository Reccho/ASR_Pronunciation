% PREAMBLE
\documentclass[12pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{tipa}
\usepackage[margin=1in]{geometry}
\usepackage[parfill]{parskip}
\usepackage{hyperref}
\title{Pronunciation Checking with ASR}
\author{Evan Nichols}
\date{MFF 2020/2021}

\begin{document}
\maketitle

\section*{Main Idea}
The goal of this project is to present the user with a simple application that will allow them to submit recorded speech and receive a score based on their pronunciation. The application will be built using Flask, a web application framework in Python. The main computational part of the app will be using NVIDIA's NeMo toolkit, which provides the neural network module that actually grades the speech input. Structurally, the application consists of a web page run in JavaScript, and a back end in Python, which uses the NN to process speech. 

\section*{Interface}
When looking at the webpage the user will see several elements:
\begin{itemize}
\item A drop-down list selection of XML library files that hold the speech prompt phrases.

\item A numbered selection which pulls a phrase from the selected XML library. Here, the selected number corresponds to the element id of the phrase in the library.

\item \textbf{Record} begins recording. The user should read the selected phrase (displayed in the text box above) aloud and press \textbf{Stop} when finished. The audio player to the right allows the user to preview their recorded audio, and the user is able to overwrite the current input by simply hitting Record again. Upon recording, the app also creates a file '/temp/sample.txt' which contains the prompt.
Upon clicking Stop, a spectrogram of the audio is displayed at the bottom of the page, and the app saves the input audio to '/temp/input.wav'. 
\textit{The app requires access to the user's microphone.}

\item \textbf{Grade} passes the user's audio input to the app's back end. The audio is written to a file, this file is formatted and its duration stored, and the file's name, duration, and the phrase prompt are written to a JSON file, which consists of one line that matches the following format: \\
\hspace*{19mm} \{"audio\_filename": \textit{"file.wav"}, "duration": \textit{1.00}, "text": \textit{"Text."}\} \\
This file is fed to the neural network module, which returns a percentage score, which is displayed in the text box to the right. This function uses the 'sample.txt' and 'input.wav' temporary files.
\end{itemize}
\newpage%######################################################################



\section*{Tools \& Dependencies}
Packages and tools were installed using the \textbf{Pip} package manager for Python. 
\\The application uses the latest version of Python3.

\textbf{Flask} is an extensible "micro" web framework in Python. Here Flask is used to enable request dispatching between the front and back ends. It may be necessary to also use Flask-CORS (Cross Origin Resource Sharing) to bypass the security stops of some browsers. Chromium-based browsers, for example, will likely require this inclusion.
\\To install: \textit{pip install Flask}
\\\hspace*{19mm} \textit{pip install -U flask-cors}
%https://flask.palletsprojects.com/en/1.1.x/
%https://github.com/corydolphin/flask-cors

\textbf{NeMo} is a toolkit for applications of Conversational AI. It includes extendable collections of pre-made modules and pre-trained models for automatic speech recognition, natural language processing, and text-to-speech. NeMo can utilize NVIDIA's Tensor Cores and is able to be scaled out to multiple GPUs. The current version of NeMo requires Pytorch 1.7.1, which defines the Tensor class that allows storing and operating on multidimensional rectangular arraysof numbers on CUDA-capable NVIDIA GPUs.
\\To install: \textit{pip install Cython}
\\\hspace*{19mm} \textit{apt-get update \&\& apt-get install -y libsndfile1 ffmpeg sox}
\\\hspace*{19mm} \textit{pip install nemo-toolkit==1.0.0b3}
\\\hspace*{19mm} \textit{pip install torch torchvision torchaudio}

\textbf{ffmpeg} is a command-line tool used to convert multimedia files between formats. In this application, the command used is \textit{ffmpeg -i input\_file.* -ar 16000 -ac 1 output.wav}. Here, \textit{-ar 16000} specifies the sampling frequency (which is the same as the input stream by default) and \textit{-ac 1} specifies the number of audio channels. Reformatting the audio input with this command prepares it for processing by the neural network module. This command is always run, even when it may be redundant.
\\To install: \textit{pip install python-ffmpeg}
%https://ffmpeg.org/

\textbf{Librosa} is a Python audio library, used to create a spectrogram of the user's audio input. 
\textbf{Matplotlib} is a Python library for creating visualizations. Here it is used to draw the spectrogram of the user's audio input.
\\To install: \textit{pip install librosa}
\\\hspace*{19mm} \textit{pip install matplotlib==3.1.3}
%https://github.com/librosa/librosa
%https://matplotlib.org/

\textbf{Phonemizer} provides both a command-line tool and a Python function that allow one to phonemize text, which converts words and text to their corresponding phoneme tokens. Text can be converted according to the IPA standard and also to other alphabets.
\\To install: \textit{pip install phonemizer}
\\\hspace*{19mm} \textit{apt-get install festival espeak-ng mbrola}
%https://github.com/bootphon/phonemizer

\textbf{Google Colab} allows building Python code in modular cells, making it an excellent testing ground for this project. Colab is preferred over the similar Jupyter Notebooks in this case because Colab offers free GPU access via the CUDA package.
\newpage%######################################################################



\section*{Details on Structure}
\subsection*{Routes}
There are four main request routes which are mapped to:
\begin{itemize}
\item{\textbf{/Library}} handles all requests related to accessing the dataset(s) of phrases. Depending on the request parameter, this route will call functions that accesses the datasets in the 'lib' folder, count the phrases in a file, and return specific phrases by ID. 
\item{\textbf{/Store\_Phrase}} is called once recording ends. Here a file 'sample.txt' is created and the currently selected phrase is written to it. This file resides in the folder 'dataset' and it is used later when 'dataset.json' is created, specifically the "TEXT" section.
\item{\textbf{/Store\_Audio}} stores the user's recorded speech input as 'input.wav' (the data is in sent as 'request.data', which is written to 'input.wav'). This function also calls Spectro(), which creates a spectrogram from 'input.wav'. This spectrogram is then encoded and sent back as 'response,' which is displayed to the user via HTML.
\item{\textbf{/Grade}} prepares the dataset and then feeds it to the ASR module. First, it saves the phrase in 'sample.txt.' Then it reformats 'input.wav' and saves the new version as 'sample.wav.' The duration of this new file is also saved, and these three items (audio file, duration, phrase) are passed to a function that writes them to a .JSON file. This file is the dataset that becomes the input to the ASR.

\end{itemize}

\subsection*{Notable Functions}
\begin{itemize}
\item{\textbf{phrase\_Get(), phrase\_Num(), and getDatasets()} are the functions that act on the folder 'lib', more specifically with the .XML files inside. These functions use the \textit{xml.etree.ElementTree} library to represent an .XML file as a tree, which enables easy searching. Phrases are found by searching the tree for the selected phrase's ID.}

\item{\textbf{Spectro()} creates a spectrogram from the user's recorded speech. The function uses the \textit{librosa} and \textit{matplotlib} libraries to plot the figure and draw it. The PNG file is saved as '/temp/spectro.png.' The spectrogram is a heatmap graph, with the y-axis representing the audio frequency (Hz) and the x-axis represents the time.}

\item{\textbf{audio\_Reformat()} uses the ffmpeg command to format the audio input correctly for the ASR module. The flag \textbf{-y} forces any existing 'sample.wav' to be overwritten. \\The flags \textbf{-ar 16000} and \textbf{-ac 1} set the sample rate to 16000 Hz and set the audio channel count to 1, respectively.}

\item{\textbf{Grade()} takes the 'dataset.json' file and runs it through the ASR module. The first element of the dataset is the filename of the audio ('sample.wav'), the second is that file's duration, and the third is the text key (the selected phrase). The module processes the audio, converts it to text, and compares that with the text key. The output is the calculated Word Error Rate (WER), which is rounded to a percentage and then subtracted from 100. This is so the user is shown (as a percentage) a score that represents their success, not their error.


}

\end{itemize}
\newpage%######################################################################

\section*{Details on Neural Network Model}
This application uses NVIDIA's NeMo toolkit. Specifically, the application uses an instance of a QuartzNet convolutional neural network. This model is instantiated using the \textbf{EncDecCTCModel} class. QuartzNet is itself a version of NVIDIA's Jasper ASR network. Quartznet makes use of separable convolutions and larger filters, enabling similar performance with fewer parameters. Like other automatic speech recognition systems, the main result is the Word Error Rate (WER). The goal of the network is to bring the WER as low as possible, to get the best possible comparison results. 

The neural network makes use of a GPU for computing, which is accessed via the CUDA package. In \textbf{Grade()} there are the lines \textit{try: from torch.cuda.amp import autocast} and \textit{can\_gpu = torch.cuda.is\_available()}. Removing these lines, as well as later references to CUDA (such as \textit{x.cuda()}) allows the module to run on just a CPU, but it will be slow and is not guaranteed to finish. GPU access was found in Google Colab and the Artificial Intelligence Cluster grid at MFF.

\subsection*{Training}
The 360-hour "clean" speech dataset from the \href{http://www.openslr.org/12/}{LibriSpeech ASR corpus} was used.


\section*{Development Timeline}
\begin{itemize}

\item{Learn about ASR models and neural networks.}
\item{Learn about NeMo specifically.}
\item{Test ASR functions on Google Colab notebook.}
\item{Implement XML searching.}
\item{Assemble phrase libraries.}
\item{Shift from Colab notebook to local python app.}
\item{Design webpage using HTML.}
\item{Implement audio recording and save-to-file.}
\item{Implement Spectrogram creation w/ Librosa.}
\item{Get application to be fully functional locally (minus ASR).}
\item{Shift application to UK AIC server.}
\item{Include ASR in program, check GPU access.}
\item{Sort out permissions so that all functions work from server.}
\item{Test full functionality.}
\item{Create/get URL and make application available to outside users.}

\end{itemize}
\newpage%######################################################################



\section*{Other Details}


\section*{Possible Additions}
\begin{itemize}
\item{Add a TTS tool that allows the user to here how TTS would pronounce the prompt. This would be useful if the user encounters words they may not know, or just do not know how to pronounce. This could be done with \textbf{pyttsx3} or \textbf{gTTS API}.}

\item{Add phrases in different languages, and the necessary support from the ASR. A small menu could let users switch the UI to other languages. Phrases can be added by simply enlarging the selection of \textit{dataset.xml} files.}

\item{Allow the user to choose how their audio input is represented visually. For example, they could choose to see a wave plot instead of a spectrogram.}
\end{itemize}
\end{document}
